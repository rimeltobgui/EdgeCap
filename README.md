# ğŸ–¼ï¸ EdgeCap: Real-Time Image Captioning at the Edge

**Lightweight Image Captioning Vision-Language Model Deployment on Raspberry Pi 4B**

**EdgeCap** is a lightweight vision-language model pipeline that runs directly on a Raspberry Pi 4B (8GB RAM, Ubuntu) using a connected camera module. It combines **TinyCLIP** for image understanding with **DistilGPT2** for caption generation, leveraging PyTorch to enable efficient, real-time or near-real-time image captioning on low-power edge devices. The system also supports the full **CLIP ViT-B/32** variant for higher accuracy when needed.

---

## ğŸš€ Key Features

- âœ… Runs on Raspberry Pi 4B with Ubuntu
- ğŸ§  Vision-Language architecture: CLIP or TinyCLIP + Distilled GPT-2
- ğŸ” Supports both **full-size** and **lightweight (TinyCLIP)** CLIP backbones
- ğŸ“¸ Works with the Raspberry Pi camera module
- âš¡ Real-time inference possible with TinyCLIP on edge hardware

---

## ğŸ“¦ Model Variants

| Variant      | Visual Encoder         | Text Decoder   | Description                              |
|--------------|------------------------|----------------|------------------------------------------|
| **EdgeCap-16** | TinyCLIP-ViT-39M-16 (wkcn)    | DistilGPT2     | Fast, low-memory version for edge devices |
| **EdgeCap-32** | CLIP ViT-B/32 (OpenCLIP) | DistilGPT2     | Larger, more accurate variant             |

---

## ğŸ§ª Evaluation

Evaluated on [Flickr30k](https://huggingface.co/datasets/nlphuji/flickr30k) using:

* **BLEU**
* **CIDEr**
* **METEOR**

---

## ğŸ¤– Deployment Details

- **Device:** Raspberry Pi 4B (8GB RAM)
- **OS:** Ubuntu 22.04
- **Libraries:** PyTorch, OpenCLIP, HuggingFace Transformers
- **Camera Support:** USB / Pi camera module
- **Input Options:** Live camera feed or local image files

---

## ğŸ“‚ Project Structure

```bash
edgecap/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ tinyclip_gpt2.py        # MLP mapper + TinyCLIP + distilgpt2
â”‚   â””â”€â”€ fullclip_gpt2.py        # ViT-B/32 + distilgpt2
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_inference.py        # Generate captions
â”‚   â”œâ”€â”€ evaluate_metrics.py     # BLEU, METEOR, CIDEr
â”‚   â””â”€â”€ capture_camera.py       # Capture via Pi camera
â”œâ”€â”€ data/
â”‚   â””â”€â”€ flickr_subset.json
â””â”€â”€ README.md
```

---

## ğŸ“ˆ Example Output

| Image                   | Caption                              |
| ----------------------- | ------------------------------------ |
| ![soccer](example1.jpg) | "Soccer players on a field"          |
| ![dog](example2.jpg)    | "A golden retriever chewing a treat" |

---

## ğŸ’¡ Future Enhancements

* Quantization & pruning for faster edge inference
* Support for real-time camera streaming
* Model distillation and further lightweighting

---

## ğŸ™ Acknowledgements

This project leverages the following open-source contributions:

* [OpenCLIP](https://github.com/mlfoundations/open_clip) for CLIP and TinyCLIP models
* [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) for `distilgpt2`
* [PyTorch](https://pytorch.org/) for model implementation and deployment
* [Flickr30k Dataset](https://huggingface.co/datasets/nlphuji/flickr30k) for evaluation
* [pycocoevalcap](https://github.com/tylin/coco-caption) for evaluation metrics

We thank the maintainers of these libraries and datasets for their invaluable work.
